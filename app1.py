import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
import joblib

class Predictor:
    def __init__(self, scaler_path, svc_path):
        self.scaler = joblib.load(scaler_path)
        self.model = joblib.load(svc_path)
        self.static_cols = ["äº§å“è´¨é‡æŒ‡æ ‡_Sn%", "æ·»åŠ æ¯”ä¾‹", "ä¸€ç”²%"]
        self.time_series_cols = ["é»„åº¦å€¼_3min", "6min", "9min", "12min", "15min", "18min", "21min", "24min"]
        self.imputer = SimpleImputer(strategy="mean")

    def _truncate(self, df):
        ts_data = df[self.time_series_cols].iloc[0]
        if ts_data.notna().any():
            last_valid_idx = ts_data.last_valid_index()
            if last_valid_idx:
                truncate_pos = self.time_series_cols.index(last_valid_idx) + 1
                for col in self.time_series_cols[truncate_pos:]:
                    df[col] = np.nan
        return df

    def _extract_time_series_features(self, df):
        ts_series = df[self.time_series_cols].iloc[0].dropna()
        if ts_series.empty:
            raise ValueError("æ—¶é—´åºåˆ—æ•°æ®å…¨ä¸ºç©ºï¼Œæ— æ³•æå–ç‰¹å¾")
        
        ts_filled = self.imputer.fit_transform(ts_series.values.reshape(-1, 1)).flatten()
        ts_series = pd.Series(ts_filled, index=ts_series.index)
        
        features = {
            'seq_length': len(ts_series),
            'max_value': ts_series.max(),
            'mean_value': ts_series.mean(),
            'min_value': ts_series.min(),
            'std_value': ts_series.std(),
            'trend': (ts_series.iloc[-1] - ts_series.iloc[0]) / len(ts_series),
            'range_value': ts_series.max() - ts_series.min(),
            'autocorr': ts_series.autocorr() if len(ts_series) > 1 else 0
        }
        return pd.DataFrame([features]), None

    def predict_one(self, sample):
        try:
            # æ„é€ è¾“å…¥æ•°æ®
            df = pd.DataFrame([sample], columns=self.static_cols + self.time_series_cols)
            
            # é¢„å¤„ç†
            df = self._truncate(df)
            
            # æå–ç‰¹å¾
            static_data = {col: df[col].iloc[0] for col in self.static_cols}
            eng_df, _ = self._extract_time_series_features(df)
            combined = pd.concat([pd.DataFrame([static_data]), eng_df], axis=1)
            
            # ç»´åº¦éªŒè¯
            if combined.shape[1] != self.scaler.n_features_in_:
                raise ValueError(
                    f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼å½“å‰ï¼š{combined.shape[1]}ï¼Œé¢„æœŸï¼š{self.scaler.n_features_in_}\n"
                    f"å½“å‰ç‰¹å¾ï¼š{combined.columns.tolist()}\n"
                    f"é¢„æœŸç‰¹å¾ï¼š{self.scaler.feature_names_in_.tolist()}"
                )
            
            # é¢„æµ‹
            X_scaled = self.scaler.transform(combined)
            return self.model.predict(X_scaled)[0]
        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥ï¼Œé”™è¯¯å †æ ˆï¼š{str(e)}")
            return -1  # æ˜ç¡®è¿”å›é”™è¯¯ä»£ç 
import streamlit as st
import pandas as pd
import numpy as np
import joblib
import base64
import random
from deap import base, creator, tools, algorithms

# è¾…åŠ©å‡½æ•°ï¼šå›¾ç‰‡è½¬base64
def image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode()

# é¡µé¢é…ç½®
image_path = "å›¾ç‰‡1.png"
icon_base64 = image_to_base64(image_path)
st.set_page_config(
    page_title="èšä¸™çƒ¯LOIå’ŒTSæ¨¡å‹",
    layout="wide",
    page_icon=f"data:image/png;base64,{icon_base64}"
)

# é¡µé¢æ ‡é¢˜æ ·å¼
width = 200
height = int(158 * (width / 507))
st.markdown(
    f"""
    <h1 style="display: flex; align-items: center;">
        <img src="data:image/png;base64,{icon_base64}" style="width: {width}px; height: {height}px; margin-right: 15px;" />
        é˜»ç‡ƒèšåˆç‰©å¤åˆææ–™æ™ºèƒ½è®¾è®¡å¹³å°
    </h1>
    """, 
    unsafe_allow_html=True
)


# ä¾§è¾¹æ ä¸»å¯¼èˆª
page = st.sidebar.selectbox(
    "ğŸ”§ ä¸»åŠŸèƒ½é€‰æ‹©",
    ["æ€§èƒ½é¢„æµ‹", "é…æ–¹å»ºè®®"],
    key="main_nav"
)

# å­åŠŸèƒ½é€‰æ‹©ï¼ˆä»…åœ¨é…æ–¹å»ºè®®æ—¶æ˜¾ç¤ºï¼‰
sub_page = None
if page == "é…æ–¹å»ºè®®":
    sub_page = st.sidebar.selectbox(
        "ğŸ”§ å­åŠŸèƒ½é€‰æ‹©",
        ["é…æ–¹ä¼˜åŒ–", "æ·»åŠ å‰‚æ¨è"],
        key="sub_nav"
    )

# å•ä½ç±»å‹é€‰æ‹©ï¼ˆåŠ¨æ€æ˜¾ç¤ºï¼‰
if page == "æ€§èƒ½é¢„æµ‹" or (page == "é…æ–¹å»ºè®®" and sub_page == "é…æ–¹ä¼˜åŒ–"):
    fraction_type = st.sidebar.radio(
        "ğŸ“ å•ä½ç±»å‹",
        ["è´¨é‡", "è´¨é‡åˆ†æ•°", "ä½“ç§¯åˆ†æ•°"],
        key="unit_type"
    )
else:
    fraction_type = None

# åŠ è½½æ¨¡å‹
@st.cache_resource
def load_models():
    loi_data = joblib.load("model_and_scaler_loi.pkl")
    ts_data = joblib.load("model_and_scaler_ts1.pkl")
    return {
        "loi_model": loi_data["model"],
        "loi_scaler": loi_data["scaler"],
        "ts_model": ts_data["model"],
        "ts_scaler": ts_data["scaler"],
        "loi_features": pd.read_excel("trainrg3.xlsx").drop(columns="LOI", errors='ignore').columns.tolist(),
        "ts_features": pd.read_excel("trainrg3TS.xlsx").drop(columns="TS", errors='ignore').columns.tolist(),
    }
models = load_models()

# è·å–å•ä½
def get_unit(fraction_type):
    if fraction_type == "è´¨é‡":
        return "g"
    elif fraction_type == "è´¨é‡åˆ†æ•°":
        return "wt%"
    elif fraction_type == "ä½“ç§¯åˆ†æ•°":
        return "vol%"

# ä¿è¯PPåœ¨é¦–åˆ—
def ensure_pp_first(features):
    if "PP" in features:
        features.remove("PP")
    return ["PP"] + sorted(features)

# æ€§èƒ½é¢„æµ‹é¡µé¢
if page == "æ€§èƒ½é¢„æµ‹":
    st.subheader("ğŸ”® æ€§èƒ½é¢„æµ‹ï¼šåŸºäºé…æ–¹é¢„æµ‹LOIå’ŒTS")
    
    # åŠ¨æ€ç”Ÿæˆè¾“å…¥æ¡†
    input_values = {}
    features = ensure_pp_first(sorted(set(models["loi_features"] + models["ts_features"])))
    cols = st.columns(2)
    
    for i, feature in enumerate(features):
        with cols[i % 2]:
            unit = get_unit(fraction_type)
            input_values[feature] = st.number_input(
                f"{feature} ({unit})",
                min_value=0.0,
                max_value=100.0,
                value=50.0 if feature == "PP" else 0.0,
                step=0.1
            )

    # è¾“å…¥éªŒè¯
    total = sum(input_values.values())
    is_only_pp = all(v == 0 for k, v in input_values.items() if k != "PP")
    
    with st.expander("âœ… è¾“å…¥éªŒè¯"):
        if fraction_type == "ä½“ç§¯åˆ†æ•°":
            if abs(total - 100.0) > 1e-6:
                st.error(f"â— ä½“ç§¯åˆ†æ•°çš„æ€»å’Œå¿…é¡»ä¸º100%ï¼ˆå½“å‰ï¼š{total:.2f}%ï¼‰")
            else:
                st.success("ä½“ç§¯åˆ†æ•°æ€»å’ŒéªŒè¯é€šè¿‡")
        elif fraction_type == "è´¨é‡åˆ†æ•°":
            if abs(total - 100.0) > 1e-6:
                st.error(f"â— è´¨é‡åˆ†æ•°çš„æ€»å’Œå¿…é¡»ä¸º100%ï¼ˆå½“å‰ï¼š{total:.2f}%ï¼‰")
            else:
                st.success("è´¨é‡åˆ†æ•°æ€»å’ŒéªŒè¯é€šè¿‡")
        else:
            st.success("æˆåˆ†æ€»å’ŒéªŒè¯é€šè¿‡")
            if is_only_pp:
                st.info("æ£€æµ‹åˆ°çº¯PPé…æ–¹")

    if st.button("ğŸš€ å¼€å§‹é¢„æµ‹", type="primary"):
        if fraction_type == "ä½“ç§¯åˆ†æ•°" and abs(total - 100.0) > 1e-6:
            st.error("é¢„æµ‹ä¸­æ­¢ï¼šä½“ç§¯åˆ†æ•°çš„æ€»å’Œå¿…é¡»ä¸º100%")
            st.stop()
        elif fraction_type == "è´¨é‡åˆ†æ•°" and abs(total - 100.0) > 1e-6:
            st.error("é¢„æµ‹ä¸­æ­¢ï¼šè´¨é‡åˆ†æ•°çš„æ€»å’Œå¿…é¡»ä¸º100%")
            st.stop()

        # å•ä½è½¬æ¢å¤„ç†
        if fraction_type == "ä½“ç§¯åˆ†æ•°":
            # ä½“ç§¯åˆ†æ•°è½¬åŒ–ä¸ºè´¨é‡åˆ†æ•°
            vol_values = np.array([input_values[f] for f in features])
            mass_values = vol_values  # å‡è®¾ä½“ç§¯åˆ†æ•°ä¸è´¨é‡åˆ†æ•°ç›´æ¥ç›¸ç­‰
            total_mass = mass_values.sum()
            input_values = {f: (mass_values[i]/total_mass)*100 for i, f in enumerate(features)}
        
        # å¦‚æœæ˜¯çº¯PPé…æ–¹ï¼Œç›´æ¥è¿›è¡ŒLOIå’ŒTSé¢„æµ‹
        if is_only_pp:
            loi_pred = 17.5  # å‡è®¾PPé…æ–¹LOIä¸º17.5%
            ts_pred = 35.0  # å‡è®¾PPé…æ–¹TSä¸º35 MPa
        else:
            # LOIé¢„æµ‹
            loi_input = np.array([[input_values[f] for f in models["loi_features"]]])
            loi_scaled = models["loi_scaler"].transform(loi_input)
            loi_pred = models["loi_model"].predict(loi_scaled)[0]
        
            # TSé¢„æµ‹
            ts_input = np.array([[input_values[f] for f in models["ts_features"]]])
            ts_scaled = models["ts_scaler"].transform(ts_input)
            ts_pred = models["ts_model"].predict(ts_scaled)[0]
        
        # æ˜¾ç¤ºç»“æœ
        col1, col2 = st.columns(2)
        with col1:
            st.metric(label="LOIé¢„æµ‹å€¼", value=f"{loi_pred:.2f}%")
        with col2:
            st.metric(label="TSé¢„æµ‹å€¼", value=f"{ts_pred:.2f} MPa")

elif page == "é…æ–¹å»ºè®®":
    sub_page = st.sidebar.selectbox("ğŸ”§ é€‰æ‹©åŠŸèƒ½", ["","é…æ–¹ä¼˜åŒ–", "æ·»åŠ å‰‚æ¨è"])
    if sub_page == "é…æ–¹ä¼˜åŒ–":
        st.subheader("ğŸ§ª é…æ–¹å»ºè®®ï¼šæ ¹æ®æ€§èƒ½åæ¨é…æ–¹")
    
        # ç›®æ ‡è¾“å…¥
        col1, col2 = st.columns(2)
        with col1:
            target_loi = st.number_input("ç›®æ ‡LOIå€¼ï¼ˆ%ï¼‰", min_value=10.0, max_value=50.0, value=25.0, step=0.1)
        with col2:
            target_ts = st.number_input("ç›®æ ‡TSå€¼ï¼ˆMPaï¼‰", min_value=0.0, max_value=100.0, value=50.0, step=0.1)
        
        # é—ä¼ ç®—æ³•å‚æ•°
        with st.expander("âš™ï¸ ç®—æ³•å‚æ•°è®¾ç½®"):
            pop_size = st.number_input("ç§ç¾¤æ•°é‡", 50, 500, 200)
            n_gen = st.number_input("è¿­ä»£ä»£æ•°", 10, 100, 50)
            cx_prob = st.slider("äº¤å‰æ¦‚ç‡", 0.1, 1.0, 0.7)
            mut_prob = st.slider("å˜å¼‚æ¦‚ç‡", 0.1, 1.0, 0.2)
    
        if st.button("ğŸ” å¼€å§‹ä¼˜åŒ–", type="primary"):
            # åˆå§‹åŒ–é—ä¼ ç®—æ³•
            creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
            creator.create("Individual", list, fitness=creator.FitnessMin)
            
            toolbox = base.Toolbox()
            all_features = ensure_pp_first(list(set(models["loi_features"] + models["ts_features"])))
            n_features = len(all_features)
            
            # ç”Ÿæˆæ»¡è¶³å’Œä¸º100çš„é…æ–¹
            def generate_individual():
                # éšæœºç”Ÿæˆä¸€ä¸ªå’Œä¸º100çš„é…æ–¹
                individual = [random.uniform(0, 100) for _ in range(n_features)]
                total = sum(individual)
                # ä¿è¯æ€»å’Œä¸º100ï¼Œä¸”ä¸å«è´Ÿå€¼
                return [max(0, x / total * 100) for x in individual]
            
            toolbox.register("individual", tools.initIterate, creator.Individual, generate_individual)
            toolbox.register("population", tools.initRepeat, list, toolbox.individual)
            
            def evaluate(individual):
                # å•ä½è½¬æ¢å¤„ç†
                if fraction_type == "ä½“ç§¯åˆ†æ•°":
                    # è½¬æ¢ä¸ºè´¨é‡åˆ†æ•°
                    vol_values = np.array(individual)
                    mass_values = vol_values  # ç›´æ¥ä½¿ç”¨ä½“ç§¯åˆ†æ•°æ¯”ä¾‹è¡¨ç¤ºè´¨é‡åˆ†æ•°
                    total_mass = mass_values.sum()
                    if total_mass == 0:
                        return (1e6,)
                    mass_percent = (mass_values / total_mass) * 100
                else:
                    total = sum(individual)
                    if total == 0:
                        return (1e6,)
                    mass_percent = np.array(individual) / total * 100
                
                # PPçº¦æŸ
                pp_index = all_features.index("PP")
                pp_content = mass_percent[pp_index]
                if pp_content < 50:  # PPå«é‡è¿‡ä½æƒ©ç½š
                    return (1e6,)
                
                # LOIè®¡ç®—
                loi_input = mass_percent[:len(models["loi_features"])]
                loi_scaled = models["loi_scaler"].transform([loi_input])
                loi_pred = models["loi_model"].predict(loi_scaled)[0]
                loi_error = abs(target_loi - loi_pred)
                
                # TSè®¡ç®—
                ts_input = mass_percent[:len(models["ts_features"])]
                ts_scaled = models["ts_scaler"].transform([ts_input])
                ts_pred = models["ts_model"].predict(ts_scaled)[0]
                ts_error = abs(target_ts - ts_pred)
                total = sum(mass_percent)
                if abs(total - 100) > 1e-6:
                    return (1e6,)
                return (loi_error + ts_error,)
            
            toolbox.register("mate", tools.cxBlend, alpha=0.5)
            toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)
            toolbox.register("select", tools.selTournament, tournsize=3)
            toolbox.register("evaluate", evaluate)
            
            population = toolbox.population(n=pop_size)
            algorithms.eaSimple(population, toolbox, cxpb=cx_prob, mutpb=mut_prob, ngen=n_gen, verbose=False)
            
            # é€‰æ‹©10ä¸ªé…æ–¹å¹¶ç¡®ä¿æ¯ä¸ªé…æ–¹çš„æ€»å’Œä¸º100
            best_individuals = tools.selBest(population, 10)
            best_values = []
            for individual in best_individuals:
                # ç¡®ä¿æ¯ä¸ªé…æ–¹çš„æ€»å’Œä¸º100ï¼Œå¹¶ä¿®æ­£è´Ÿå€¼
                total = sum(individual)
                best_values.append([round(max(0, i / total * 100), 2) for i in individual])
    
            # è¾“å‡ºä¼˜åŒ–ç»“æœ
            result_df = pd.DataFrame(best_values, columns=all_features)
            
            # æ·»åŠ å•ä½åˆ—
            units = [get_unit(fraction_type) for _ in all_features]
            result_df.columns = [f"{col} ({unit})" for col, unit in zip(result_df.columns, units)]
            
            st.write(result_df)
    elif sub_page == "æ·»åŠ å‰‚æ¨è":
        st.subheader("ğŸ§ª æ·»åŠ å‰‚æ™ºèƒ½æ¨è")
    
        @st.cache_resource
        def load_predictor():
            return Predictor(
                scaler_path="scaler_fold_1.pkl",
                svc_path="svc_fold_1.pkl"
            )
    
        predictor = load_predictor()  # æ³¨æ„è¿™é‡Œä¿®æ­£äº†æ‹¼å†™é”™è¯¯
    
        # åˆ›å»ºè¾“å…¥è¡¨å•
        with st.form("additive_form"):
            st.markdown("### åŸºç¡€å‚æ•°")
            col_static = st.columns(3)
            with col_static[0]:
                add_ratio = st.number_input("æ·»åŠ æ¯”ä¾‹ (%)", 0.0, 100.0, 5.0, step=0.1)
            with col_static[1]:
                sn_percent = st.number_input("Snå«é‡ (%)", 0.0, 100.0, 98.5, step=0.1)
            with col_static[2]:
                yijia_percent = st.number_input("ä¸€ç”²èƒºå«é‡ (%)", 0.0, 100.0, 0.5, step=0.1)
    
            st.markdown("### æ—¶åºå‚æ•°ï¼ˆé»„åº¦å€¼éšæ—¶é—´å˜åŒ–ï¼‰")
    
            time_points = [
                ("3min", 1.2), ("6min", 1.5), ("9min", 1.8),
                ("12min", 2.0), ("15min", 2.2), ("18min", 2.5),
                ("21min", 2.8), ("24min", 3.0)
            ]
    
            yellow_values = {}
            cols = st.columns(4)
            for idx, (time, default) in enumerate(time_points):
                with cols[idx % 4]:
                    yellow_values[time] = st.number_input(
                        f"{time} é»„åº¦å€¼",
                        min_value=0.0,
                        value=default,
                        step=0.1,
                        key=f"yellow_{time}"
                    )
    
            submitted = st.form_submit_button("ç”Ÿæˆæ¨èæ–¹æ¡ˆ")
    
            if submitted:
                try:
                    # æ„å»ºè¾“å…¥æ ·æœ¬ï¼ˆé¡ºåºä¸ç±»å®šä¹‰ä¸€è‡´ï¼‰
                    sample = [
                        sn_percent,    # å¯¹åº” static_cols[0]
                        add_ratio,     # å¯¹åº” static_cols[1]
                        yijia_percent, # å¯¹åº” static_cols[2]
                        yellow_values["3min"],
                        yellow_values["6min"],
                        yellow_values["9min"],
                        yellow_values["12min"],
                        yellow_values["15min"],
                        yellow_values["18min"],
                        yellow_values["21min"],
                        yellow_values["24min"]
                    ]
    
                    prediction = predictor.predict_one(sample)
    
                    # ç»“æœæ˜ å°„è¡¨
                    result_map = {
                        1: {"name": "æ— "},
                        2: {"name": "æ°¯åŒ–çŸ³èœ¡"},
                        3: {"name": "EA12ï¼ˆè„‚è‚ªé…¸å¤åˆé†‡é…¯ï¼‰"},
                        4: {"name": "EA15ï¼ˆå¸‚å”®æ¶²ä½“é’™é”Œç¨³å®šå‰‚ï¼‰"},
                        5: {"name": "EA16ï¼ˆç¯æ°§å¤§è±†æ²¹ï¼‰"},
                        6: {"name": "G70Lï¼ˆå¤šå®˜èƒ½å›¢çš„è„‚è‚ªé…¸å¤åˆé…¯æ··åˆç‰©ï¼‰"},
                        7: {"name": "EA6ï¼ˆäºšç£·é…¸é…¯ï¼‰"}
                    }
    
                    st.success("### æ¨èç»“æœ")
                    col1, col2 = st.columns([1, 2])
                    with col1:
                        st.metric("æ¨èæ·»åŠ å‰‚ç±»å‹", result_map[prediction]["name"])
                    with col2:
                        st.markdown(f"""
                        **æ¨èæ·»åŠ å‰‚**: {result_map[prediction]["name"]}
                        """)
                    
                except Exception as e:
                    st.error(f"é¢„æµ‹æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")

