import streamlit as st
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler
from deap import base, creator, tools, algorithms
import base64

# Function to convert image to base64
def image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode()

# è®¾ç½®é¡µé¢é…ç½®ï¼ˆä¿æŒåŸæ ·ï¼Œå›¾æ ‡ä¾ç„¶æ˜¯æ˜¾ç¤ºåœ¨æµè§ˆå™¨æ ‡ç­¾é¡µä¸­ï¼‰
image_path = "å›¾ç‰‡1.png"  # ä½¿ç”¨ä¸Šä¼ çš„å›¾ç‰‡è·¯å¾„
icon_base64 = image_to_base64(image_path)  # è½¬æ¢ä¸º base64

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(page_title="èšä¸™çƒ¯LOIæ¨¡å‹", layout="wide", page_icon=f"data:image/png;base64,{icon_base64}")

# å›¾æ ‡åŸå§‹å°ºå¯¸ï¼š507x158ï¼Œè®¡ç®—å‡ºæ¯”ä¾‹
width = 200  # è®¾ç½®å›¾æ ‡çš„å®½åº¦ä¸º100px
height = int(158 * (width / 507))  # è®¡ç®—ä¿æŒæ¯”ä¾‹åçš„é«˜åº¦

# åœ¨é¡µé¢ä¸Šæ’å…¥å›¾æ ‡ä¸æ ‡é¢˜
st.markdown(
    f"""
    <h1 style="display: flex; align-items: center;">
        <img src="data:image/png;base64,{icon_base64}" style="width: {width}px; height: {height}px; margin-right: 15px;" />
        é˜»ç‡ƒèšåˆç‰©å¤åˆææ–™æ™ºèƒ½è®¾è®¡å¹³å°
    </h1>
    """, 
    unsafe_allow_html=True
)

page = st.sidebar.selectbox("ğŸ”§ é€‰æ‹©åŠŸèƒ½", ["æ€§èƒ½é¢„æµ‹", "é…æ–¹å»ºè®®"])

# åŠ è½½æ¨¡å‹ä¸ç¼©æ”¾å™¨
data = joblib.load("model_and_scaler_loi.pkl")
model = data["model"]
scaler = data["scaler"]

df = pd.read_excel("trainrg3.xlsx")
feature_names = df.columns.tolist()
if "LOI" in feature_names:
    feature_names.remove("LOI")

unit_type = st.radio("ğŸ“ è¯·é€‰æ‹©é…æ–¹è¾“å…¥å•ä½", ["è´¨é‡ (g)", "è´¨é‡åˆ†æ•° (wt%)", "ä½“ç§¯åˆ†æ•° (vol%)"], horizontal=True)

if page == "æ€§èƒ½é¢„æµ‹":
    st.subheader("ğŸ”¬ æ­£å‘é¢„æµ‹ï¼šé…æ–¹ â†’ LOI")

    with st.form("input_form"):
        user_input = {}
        total = 0
        cols = st.columns(3)
        for i, name in enumerate(feature_names):
            unit_label = {
                "è´¨é‡ (g)": "g",
                "è´¨é‡åˆ†æ•° (wt%)": "wt%",
                "ä½“ç§¯åˆ†æ•° (vol%)": "vol%"
            }[unit_type]
            val = cols[i % 3].number_input(f"{name} ({unit_label})", value=0.0, step=0.1 if "è´¨é‡" in unit_type else 0.01)
            user_input[name] = val
            total += val

        submitted = st.form_submit_button("ğŸ“Š å¼€å§‹é¢„æµ‹")

    if submitted:
        # åˆ¤æ–­æ€»å’Œæ˜¯å¦æ»¡è¶³ä¸º100
        if unit_type != "è´¨é‡ (g)" and abs(total - 100) > 1e-3:
            st.warning("âš ï¸ é…æ–¹åŠ å’Œä¸ä¸º100ï¼Œæ— æ³•é¢„æµ‹ã€‚è¯·ç¡®ä¿æ€»å’Œä¸º100åå†è¿›è¡Œé¢„æµ‹ã€‚")
        else:
            # è‹¥æ˜¯åˆ†æ•°å•ä½ï¼Œåˆ™å†å½’ä¸€åŒ–ä¸€é
            if unit_type == "è´¨é‡ (g)" and total > 0:  # åˆ¤æ–­æ˜¯å¦ä¸ºè´¨é‡å•ä½
                # å°†æ¯ä¸ªæˆåˆ†çš„è´¨é‡è½¬æ¢ä¸ºè´¨é‡åˆ†æ•°
                user_input = {k: (v / total) * 100 for k, v in user_input.items()}  # å½’ä¸€åŒ–ä¸ºè´¨é‡åˆ†æ•°

            elif unit_type != "è´¨é‡ (g)" and total > 0:
                user_input = {k: v / total * 100 for k, v in user_input.items()}  # ç¡®ä¿æ€»å’Œä¸º100

            # æ£€æŸ¥æ˜¯å¦ä»…è¾“å…¥äº†PPï¼Œå¹¶ä¸”PPä¸º100
            if np.all([user_input.get(name, 0) == 0 for name in feature_names if name != "PP"]) and user_input.get("PP", 0) == 100:
                # å¦‚æœåªè¾“å…¥äº†PPä¸”PPä¸º100ï¼Œå¼ºåˆ¶è¿”å›LOI=17.5
                st.markdown("### ğŸ¯ é¢„æµ‹ç»“æœ")
                st.metric(label="æé™æ°§æŒ‡æ•° (LOI)", value="17.5 %")
            else:
                input_array = np.array([list(user_input.values())])
                input_scaled = scaler.transform(input_array)
                prediction = model.predict(input_scaled)[0]

                st.markdown("### ğŸ¯ é¢„æµ‹ç»“æœ")
                st.metric(label="æé™æ°§æŒ‡æ•° (LOI)", value=f"{prediction:.2f} %")

elif page == "é€†å‘è®¾è®¡":
    # ç”¨æˆ·è¾“å…¥çš„ç›®æ ‡ LOI éœ€è¦åœ¨10åˆ°40ä¹‹é—´
    target_loi = st.number_input("ğŸ¯ è¯·è¾“å…¥ç›®æ ‡ LOI å€¼ (%)", value=20.0, step=0.1, min_value=10.0, max_value=40.0)

    # æ£€æŸ¥ç›®æ ‡ LOI æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
    if target_loi < 10 or target_loi > 40:
        st.warning("âš ï¸ ç›®æ ‡ LOI å€¼å¿…é¡»åœ¨ 10 åˆ° 40 ä¹‹é—´ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
    else:
        st.write("ğŸ”„ æ­£åœ¨è¿›è¡Œé€†å‘è®¾è®¡ï¼Œè¯·ç¨ç­‰...")

        # é…æ–¹èŒƒå›´
        bounds = {
            feature: (0, 100) for feature in feature_names
        }
        pp_index = feature_names.index("PP")
        bounds["PP"] = (50, 100)  # è®¾å®šPPçš„èŒƒå›´æ›´é«˜ä¸€äº›

        # é—ä¼ ç®—æ³•ä¼˜åŒ–ï¼ˆDEAPï¼‰
        creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
        creator.create("Individual", list, fitness=creator.FitnessMin)

        def evaluate(individual):
            individual_norm = np.array(individual) / sum(individual) * 100  # ç¡®ä¿æ€»å’Œä¸º100
            individual_norm = individual_norm.round(3)
            
            # ç¡®ä¿PPçš„æ¯”ä¾‹è¾ƒå¤§ï¼Œæƒ©ç½šPPæ¯”ä¾‹è¾ƒå°çš„ä¸ªä½“
            pp_percentage = individual_norm[pp_index]
            penalty = 0
            if pp_percentage < 50:
                penalty = 10  # ç»™è¾ƒå°çš„PPæ¯”ä¾‹è¾ƒå¤§æƒ©ç½š

            scaled_input = scaler.transform([individual_norm])
            prediction = model.predict(scaled_input)[0]
            return abs(prediction - target_loi) + penalty,

        toolbox = base.Toolbox()
        toolbox.register("attr_float", np.random.uniform, 0.01, 1.0)
        toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, n=len(feature_names))
        toolbox.register("population", tools.initRepeat, list, toolbox.individual)
        toolbox.register("evaluate", evaluate)
        toolbox.register("mate", tools.cxBlend, alpha=0.5)
        toolbox.register("mutate", tools.mutGaussian, mu=0.0, sigma=0.1, indpb=0.2)
        toolbox.register("select", tools.selTournament, tournsize=3)

        population = toolbox.population(n=50)
        algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2, ngen=50, verbose=True)

        best_individual = tools.selBest(population, 1)[0]
        best_individual_norm = np.array(best_individual) / sum(best_individual) * 100  # ç¡®ä¿æ€»å’Œä¸º100
        best_prediction = model.predict(scaler.transform([best_individual_norm]))[0]

        st.success("ğŸ‰ æˆåŠŸåæ¨é…æ–¹ï¼")
        st.metric("é¢„æµ‹ LOI", f"{best_prediction:.2f} %")

        # æ˜¾ç¤ºæœ€ä¼˜é…æ–¹
        df_result = pd.DataFrame([best_individual_norm], columns=feature_names)
        df_result.columns = [f"{col} (wt%)" for col in df_result.columns]

        st.markdown("### ğŸ“‹ æœ€ä¼˜é…æ–¹å‚æ•°")
        st.dataframe(df_result.round(2))
