import streamlit as st
import pandas as pd
import numpy as np
import joblib
import base64
import random
from deap import base, creator, tools, algorithms

# è¾…åŠ©å‡½æ•°ï¼šå›¾ç‰‡è½¬base64
def image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode()

# é¡µé¢é…ç½®
image_path = "å›¾ç‰‡1.png"
icon_base64 = image_to_base64(image_path)
st.set_page_config(
    page_title="èšä¸™çƒ¯LOIå’ŒTSæ¨¡å‹",
    layout="wide",
    page_icon=f"data:image/png;base64,{icon_base64}"
)

# é¡µé¢æ ‡é¢˜æ ·å¼
width = 200
height = int(158 * (width / 507))
st.markdown(
    f"""
    <h1 style="display: flex; align-items: center;">
        <img src="data:image/png;base64,{icon_base64}" style="width: {width}px; height: {height}px; margin-right: 15px;" />
        é˜»ç‡ƒèšåˆç‰©å¤åˆææ–™æ™ºèƒ½è®¾è®¡å¹³å°
    </h1>
    """, 
    unsafe_allow_html=True
)

# ä¾§è¾¹æ å¯¼èˆª
page = st.sidebar.selectbox("ğŸ”§ é€‰æ‹©åŠŸèƒ½", ["æ€§èƒ½é¢„æµ‹", "é…æ–¹å»ºè®®"])

# åŠ è½½LOIæ¨¡å‹å’ŒScaler
loi_data = joblib.load("model_and_scaler_loi.pkl")
loi_model = loi_data["model"]
loi_scaler = loi_data["scaler"]

# åŠ è½½TSæ¨¡å‹å’ŒScaler
ts_data = joblib.load("model_and_scaler_ts1.pkl")
ts_model = ts_data["model"]
ts_scaler = ts_data["scaler"]

# åŠ è½½è®­ç»ƒæ•°æ®ï¼Œè·å–ç‰¹å¾åç§°
df_loi = pd.read_excel("trainrg3.xlsx")
df_ts = pd.read_excel("trainrg3TS.xlsx")

loi_feature_names = df_loi.columns.tolist()
ts_feature_names = df_ts.columns.tolist()

# ç§»é™¤LOIå’ŒTSåˆ—ï¼Œå¾—åˆ°ç‰¹å¾åç§°
if "LOI" in loi_feature_names:
    loi_feature_names.remove("LOI")

if "TS" in ts_feature_names:
    ts_feature_names.remove("TS")

# æ€§èƒ½é¢„æµ‹é¡µé¢
if page == "æ€§èƒ½é¢„æµ‹":
    st.subheader("ğŸ”® æ€§èƒ½é¢„æµ‹ï¼šåŸºäºé…æ–¹é¢„æµ‹LOIå’ŒTS")

    # åˆå¹¶LOIå’ŒTSè¾“å…¥åŒºåŸŸ
    st.write("è¯·è¾“å…¥é…æ–¹ç‰¹å¾å€¼ï¼š")

    # è¾“å…¥æ‰€æœ‰ç‰¹å¾å€¼ï¼ˆç»Ÿä¸€è¾“å…¥ï¼‰
    input_data = {}
    for feature in set(loi_feature_names + ts_feature_names):  # åˆå¹¶LOIå’ŒTSçš„ç‰¹å¾
        input_data[feature] = st.number_input(f"è¯·è¾“å…¥ {feature} çš„ç‰¹å¾å€¼", value=0.0, step=0.1)

    # æ€§èƒ½é¢„æµ‹æŒ‰é’®
    if st.button("é¢„æµ‹LOIå’ŒTS"):
        # å°†è¾“å…¥æ•°æ®è½¬åŒ–ä¸ºæ•°ç»„
        input_array = np.array([list(input_data.values())])

        # LOIé¢„æµ‹ï¼šä»…é€‰æ‹©LOIç›¸å…³ç‰¹å¾
        loi_input_array = np.array([list(input_data[feature] for feature in loi_feature_names)])
        if len(loi_input_array[0]) == len(loi_feature_names):
            # æ ‡å‡†åŒ–å¹¶é¢„æµ‹LOI
            loi_input_scaled = loi_scaler.transform(loi_input_array)
            predicted_loi = loi_model.predict(loi_input_scaled)[0]
        else:
            st.error(f"LOIè¾“å…¥ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼šæœŸæœ› {len(loi_feature_names)}ï¼Œå®é™…è¾“å…¥ {len(loi_input_array[0])}")

        # TSé¢„æµ‹ï¼šä»…é€‰æ‹©TSç›¸å…³ç‰¹å¾
        ts_input_array = np.array([list(input_data[feature] for feature in ts_feature_names)])
        if len(ts_input_array[0]) == len(ts_feature_names):
            # æ ‡å‡†åŒ–å¹¶é¢„æµ‹TS
            ts_input_scaled = ts_scaler.transform(ts_input_array)
            predicted_ts = ts_model.predict(ts_input_scaled)[0]
        else:
            st.error(f"TSè¾“å…¥ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼šæœŸæœ› {len(ts_feature_names)}ï¼Œå®é™…è¾“å…¥ {len(ts_input_array[0])}")

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        if len(loi_input_array[0]) == len(loi_feature_names) and len(ts_input_array[0]) == len(ts_feature_names):
            st.success(f"é¢„æµ‹çš„LOIå€¼ä¸ºï¼š{predicted_loi:.2f} %")
            st.success(f"é¢„æµ‹çš„TSå€¼ä¸ºï¼š{predicted_ts:.2f} MPa")

# é…æ–¹å»ºè®®é¡µé¢
elif page == "é…æ–¹å»ºè®®":
    st.subheader("ğŸ§ª é…æ–¹å»ºè®®ï¼šæ ¹æ®æ€§èƒ½åæ¨é…æ–¹")
    target_loi = st.number_input("ç›®æ ‡LOIå€¼", min_value=10.0, max_value=50.0, value=25.0, step=0.1)
    target_ts = st.number_input("ç›®æ ‡TSå€¼", min_value=0.0, max_value=100.0, value=50.0, step=0.1)
    
    # é—ä¼ ç®—æ³•é…ç½®
    if not hasattr(creator, "FitnessMin"):
        creator.create("FitnessMin", base.Fitness, weights=(-1.0,))  # ç›®æ ‡æ˜¯æœ€å°åŒ–
    if not hasattr(creator, "Individual"):
        creator.create("Individual", list, fitness=creator.FitnessMin)
    
    toolbox = base.Toolbox()
    toolbox.register("attr_float", random.uniform, 0.01, 50)
    toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, n=len(loi_feature_names))  # ä½¿ç”¨LOIç‰¹å¾æ•°é‡
    toolbox.register("population", tools.initRepeat, list, toolbox.individual)
    
    def evaluate(individual):
        # å¼ºåˆ¶PPå«é‡>=50ä¸”ä¸ºæœ€å¤§å€¼
        pp_index = loi_feature_names.index("PP")
        if individual[pp_index] < 50:
            return (1000,)
        if individual[pp_index] != max(individual):
            return (1000,)
            
        # å½’ä¸€åŒ–å¤„ç†
        total = sum(individual)
        normalized = [x/total*100 for x in individual]
        
        # é¢„æµ‹LOI
        input_array = np.array([normalized])
        input_scaled = loi_scaler.transform(input_array)
        predicted_loi = loi_model.predict(input_scaled)[0]
        
        # é¢„æµ‹TS
        ts_input_scaled = ts_scaler.transform(np.array([normalized]))
        predicted_ts = ts_model.predict(ts_input_scaled)[0]

        # ç›®æ ‡å‡½æ•°ï¼šæœ€å°åŒ–LOIå’ŒTSçš„å·®è·
        return (abs(predicted_loi - target_loi) + abs(predicted_ts - target_ts),)
    
    toolbox.register("mate", tools.cxBlend, alpha=0.5)
    toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)
    toolbox.register("select", tools.selTournament, tournsize=3)
    toolbox.register("evaluate", evaluate)

    # æ‰§è¡Œé—ä¼ ç®—æ³•
    population = toolbox.population(n=10)
    result = algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2, ngen=10, verbose=False)
    
    # è¾“å‡ºæœ€ä¼˜é…æ–¹
    best_individual = tools.selBest(population, 1)[0]
    st.write("æœ€ä¼˜é…æ–¹ï¼š", best_individual)

