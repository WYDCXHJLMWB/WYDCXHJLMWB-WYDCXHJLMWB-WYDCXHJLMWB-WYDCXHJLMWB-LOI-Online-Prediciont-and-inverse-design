import pandas as pd
import numpy as np
import joblib
import streamlit as st

class Predictor:
    def __init__(self, scaler_path, svc_path):
        # åŠ è½½è®­ç»ƒå¥½çš„ scaler å’Œ svc æ¨¡å‹
        self.scaler = joblib.load(scaler_path)
        self.model = joblib.load(svc_path)
        # å®šä¹‰é™æ€ç‰¹å¾å’Œæ—¶åºç‰¹å¾çš„åˆ—åï¼ˆé¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        self.static_cols = ["äº§å“è´¨é‡æŒ‡æ ‡_Sn%", "æ·»åŠ æ¯”ä¾‹", "ä¸€ç”²%"]
        self.time_series_cols = ["é»„åº¦å€¼_3min", "6min", "9min", "12min", "15min", "18min", "21min", "24min"]

    def _truncate(self, df):
        """å¤„ç†æ—¶åºæ•°æ®æˆªæ–­ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
        time_cols = [col for col in df.columns if "min" in col.lower()]
        time_cols_ordered = [col for col in df.columns if col in time_cols]
        if time_cols_ordered:
            row = df.iloc[0][time_cols_ordered]
            if row.notna().any():
                max_idx = row.idxmax()
                max_pos = time_cols_ordered.index(max_idx)
                for col in time_cols_ordered[max_pos + 1:]:
                    df.at[df.index[0], col] = np.nan
        return df

    def _extract_features(self, df):
        """æå–å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
        # é™æ€ç‰¹å¾
        static_data = {
            col: df[col].values[0] 
            for col in self.static_cols
        }
        
        # æ—¶åºç‰¹å¾å·¥ç¨‹
        ts_cols = [col for col in df.columns if "min" in col.lower()]
        ts_series = df[ts_cols].iloc[0].dropna()
        
        eng_features = {
            'seq_length': len(ts_series),
            'max_value': ts_series.max(),
            'mean_value': ts_series.mean(),
            'min_value': ts_series.min(),
            'std_value': ts_series.std(),
            'trend': (ts_series[-1] - ts_series[0])/len(ts_series),
            'range_value': ts_series.max() - ts_series.min(),
            'autocorr': ts_series.autocorr()
        }
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        return {**static_data, **eng_features}

    def predict_one(self, sample):
        """å®Œæ•´çš„é¢„æµ‹æµç¨‹"""
        # æ„é€ å®Œæ•´è¾“å…¥DataFrameï¼ˆåŒ…å«æ‰€æœ‰åŸå§‹åˆ—ï¼‰ 
        full_cols = self.static_cols + self.time_series_cols
        df = pd.DataFrame([sample], columns=full_cols)
        
        # é¢„å¤„ç†æµç¨‹
        df = self._truncate(df)
        
        # ç‰¹å¾å·¥ç¨‹
        features = self._extract_features(df)
        feature_df = pd.DataFrame([features])[self.static_cols + self.eng_features]
        
        # éªŒè¯ç‰¹å¾ç»´åº¦
        if feature_df.shape[1] != self.scaler.n_features_in_:
            raise ValueError(
                f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼å½“å‰ï¼š{feature_df.shape[1]}ï¼Œ"
                f"éœ€è¦ï¼š{self.scaler.n_features_in_}"
            )
        
        # æ ‡å‡†åŒ– & é¢„æµ‹
        X_scaled = self.scaler.transform(feature_df)
        return self.model.predict(X_scaled)[0]
    def extract_time_series_features(df, feature_types=None, static_features=None):
        """
        æå–æ—¶åºç‰¹å¾å¹¶ä¸é™æ€ç‰¹å¾åˆå¹¶
        
        å‚æ•°:
        df: DataFrame, åŸå§‹æ•°æ®
        feature_types: list, è¦æå–çš„æ—¶åºç‰¹å¾ç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤ä¸º['seq_length', 'max_value', 'mean_value']
        static_features: list, é™æ€ç‰¹å¾çš„åˆ—ååˆ—è¡¨
        
        å¯ç”¨çš„ç‰¹å¾ç±»å‹:
        - seq_length: åºåˆ—é•¿åº¦ï¼ˆéNaNå€¼çš„æ•°é‡ï¼‰
        - max_value: æœ€å¤§å€¼
        - mean_value: å‡å€¼
        - min_value: æœ€å°å€¼
        - std_value: æ ‡å‡†å·®
        - trend: è¶‹åŠ¿ï¼ˆçº¿æ€§å›å½’æ–œç‡ï¼‰
        - range_value: æ•°å€¼èŒƒå›´ï¼ˆæœ€å¤§å€¼-æœ€å°å€¼ï¼‰
        - kurtosis: å³°åº¦
        - skewness: ååº¦
        - autocorr: ä¸€é˜¶è‡ªç›¸å…³ç³»æ•°
        
        è¿”å›:
        DataFrame: åˆå¹¶åçš„ç‰¹å¾æ•°æ®æ¡†
        time_data: åŸå§‹æ—¶åºæ•°æ®
        """
        if feature_types is None:
            feature_types = ['seq_length', 'max_value', 'mean_value']
        
        # è¯†åˆ«æ—¶åºç‰¹å¾åˆ—
        time_cols = [col for col in df.columns if 'min' in str(col).lower()]
        time_data = df[time_cols]
    
# é¡µé¢é…ç½®
image_path = "å›¾ç‰‡1.png"
icon_base64 = image_to_base64(image_path)
st.set_page_config(
    page_title="èšä¸™çƒ¯LOIå’ŒTSæ¨¡å‹",
    layout="wide",
    page_icon=f"data:image/png;base64,{icon_base64}"
)

# é¡µé¢æ ‡é¢˜æ ·å¼
width = 200
height = int(158 * (width / 507))
st.markdown(
    f"""
    <h1 style="display: flex; align-items: center;">
        <img src="data:image/png;base64,{icon_base64}" style="width: {width}px; height: {height}px; margin-right: 15px;" />
        é˜»ç‡ƒèšåˆç‰©å¤åˆææ–™æ™ºèƒ½è®¾è®¡å¹³å°
    </h1>
    """, 
    unsafe_allow_html=True
)

# ä¾§è¾¹æ å¯¼èˆª
page = st.sidebar.selectbox("ğŸ”§ é€‰æ‹©åŠŸèƒ½", ["æ€§èƒ½é¢„æµ‹", "é…æ–¹å»ºè®®"])

# åŠ è½½æ¨¡å‹
@st.cache_resource
def load_models():
    loi_data = joblib.load("model_and_scaler_loi.pkl")
    ts_data = joblib.load("model_and_scaler_ts1.pkl")
    return {
        "loi_model": loi_data["model"],
        "loi_scaler": loi_data["scaler"],
        "ts_model": ts_data["model"],
        "ts_scaler": ts_data["scaler"],
        "loi_features": pd.read_excel("trainrg3.xlsx").drop(columns="LOI", errors='ignore').columns.tolist(),
        "ts_features": pd.read_excel("trainrg3TS.xlsx").drop(columns="TS", errors='ignore').columns.tolist(),
    }

models = load_models()

# è·å–å•ä½
def get_unit(fraction_type):
    if fraction_type == "è´¨é‡":
        return "g"
    elif fraction_type == "è´¨é‡åˆ†æ•°":
        return "wt%"
    elif fraction_type == "ä½“ç§¯åˆ†æ•°":
        return "vol%"

# ä¿è¯PPåœ¨é¦–åˆ—
def ensure_pp_first(features):
    if "PP" in features:
        features.remove("PP")
    return ["PP"] + sorted(features)

# æ·»åŠ å‰‚æ¨èé¡µé¢
if page == "é…æ–¹å»ºè®®":
    sub_page = st.sidebar.selectbox("ğŸ”§ é€‰æ‹©åŠŸèƒ½", ["æ·»åŠ å‰‚æ¨è"])

    if sub_page == "æ·»åŠ å‰‚æ¨è":
        st.subheader("æ·»åŠ å‰‚æ¨è")
        
        @st.cache_resource
        def load_predictor():
            return Predictor(
                scaler_path="scaler_fold_1.pkl",
                svc_path="svc_fold_1.pkl"
            )
        
        # åŠ¨æ€ç”Ÿæˆè¾“å…¥è¡¨å•
        with st.form("additive_form"):
            col1, col2 = st.columns(2)
            
            # é™æ€å‚æ•°
            with col1:
                st.markdown("### åŸºç¡€å‚æ•°")
                sn_percent = st.number_input("Snå«é‡ (%)", 0.0, 100.0, 98.5)
                add_ratio = st.number_input("æ·»åŠ æ¯”ä¾‹ (%)", 0.0, 100.0, 5.0)  # æ¢å¤ ratio å‚æ•°
                yijia_percent = st.number_input("ä¸€ç”²èƒºå«é‡ (%)", 0.0, 100.0, 0.5)
            
            # æ—¶åºå‚æ•°
            with col2:
                st.markdown("### é»„åº¦å€¼æ—¶åºå‚æ•°")
                time_points = [3, 6, 9, 12, 15, 18, 21, 24]
                yellow_values = [
                    st.number_input(
                        f"{time}min é»„åº¦å€¼", 
                        min_value=0.0, 
                        max_value=10.0, 
                        value=1.2 + 0.3*i,
                        key=f"yellow_{time}"
                    )
                    for i, time in enumerate(time_points)
                ]
            
            submitted = st.form_submit_button("ç”Ÿæˆæ¨è")
        
        if submitted:
            try:
                # æ„å»ºå®Œæ•´è¾“å…¥æ ·æœ¬ï¼ˆé¡ºåºå¿…é¡»ä¸ç±»å®šä¹‰ä¸€è‡´ï¼ï¼‰
                sample = [
                    sn_percent,    # å¯¹åº” static_cols[0]
                    add_ratio,     # å¯¹åº” static_cols[1] æ¢å¤ ratio å‚æ•°
                    yijia_percent, # å¯¹åº” static_cols[2]
                    *yellow_values # å±•å¼€æ—¶åºå‚æ•°
                ]
                
                predictor = load_predictor()
                result = predictor.predict_one(sample)
                
                # æ¨èæ·»åŠ å‰‚ç§ç±»
                result_map = {
                    1: {"name": "æ— "},
                    2: {"name": "æ°¯åŒ–çŸ³èœ¡"},
                    3: {"name": "EA12ï¼ˆè„‚è‚ªé…¸å¤åˆé†‡é…¯ï¼‰"},
                    4: {"name": "EA15ï¼ˆå¸‚å”®æ¶²ä½“é’™é”Œç¨³å®šå‰‚ï¼‰"},
                    5: {"name": "EA16ï¼ˆç¯æ°§å¤§è±†æ²¹ï¼‰"},
                    6: {"name": "G70Lï¼ˆå¤šå®˜èƒ½å›¢çš„è„‚è‚ªé…¸å¤åˆé…¯æ··åˆç‰©ï¼‰"},
                    7: {"name": "EA6ï¼ˆäºšç£·é…¸é…¯ï¼‰"}   
                }
                
                if result not in result_map:
                    raise ValueError("æœªçŸ¥é¢„æµ‹ç»“æœ")
                
                st.success("### æ¨èæ–¹æ¡ˆ")
                st.markdown(f"""
                **æ¨èç±»å‹**: {result_map[result]['name']}
                - é€‚é…å·¥è‰ºå‚æ•°:
                  - åŠ å·¥æ¸©åº¦: 180-200â„ƒ
                  - æ··æ–™æ—¶é—´: 15-20åˆ†é’Ÿ
                """)
                
            except Exception as e:
                st.error(f"""
                ## é¢„æµ‹å¤±è´¥
                é”™è¯¯: {str(e)}
                """)
