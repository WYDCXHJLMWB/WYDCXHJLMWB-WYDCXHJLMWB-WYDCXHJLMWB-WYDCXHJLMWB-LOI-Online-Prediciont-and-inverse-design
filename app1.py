import pandas as pd
import numpy as np
import warnings
from scipy import stats
from scipy.stats import kurtosis, skew
from sklearn.impute import SimpleImputer
import joblib
import streamlit as st
import base64
import random
from deap import base, creator, tools, algorithms

# ==================== Predictorç±» ====================
class Predictor:
    def __init__(self, scaler_path, svc_path):
        self.scaler = joblib.load(scaler_path)
        self.model = joblib.load(svc_path)
        
        # æ˜ç¡®ç‰¹å¾é¡ºåºï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
        self.static_cols = ["äº§å“è´¨é‡æŒ‡æ ‡_Sn%", "æ·»åŠ æ¯”ä¾‹", "ä¸€ç”²%"]
        self.time_series_cols = [
            "é»„åº¦å€¼_3min", "6min", "9min", "12min",
            "15min", "18min", "21min", "24min"
        ]
        self.eng_features = [
            'seq_length', 'max_value', 'mean_value', 'min_value',
            'std_value', 'trend', 'range_value', 'autocorr'
        ]
        # å®šä¹‰å®Œæ•´ç‰¹å¾é¡ºåº
        self.expected_features = self.static_cols + self.eng_features
        
        # éªŒè¯scalerç»´åº¦
        if self.scaler.n_features_in_ != len(self.expected_features):
            raise ValueError(f"Scalerç‰¹å¾æ•°ä¸åŒ¹é…ï¼å½“å‰ï¼š{self.scaler.n_features_in_}ï¼Œéœ€è¦ï¼š{len(self.expected_features)}")

    def _truncate(self, df):
        """æ”¹è¿›åçš„æˆªæ–­é€»è¾‘ï¼šåŸºäºå˜åŒ–ç‡é˜ˆå€¼"""
        time_cols = sorted(  # ä¿®å¤æ‹¬å·é—­åˆé—®é¢˜
            [col for col in df.columns if "min" in col],
            key=lambda x: int(x.split('_')[-1].replace('min',''))
        )  # è¡¥å…¨è¿™ä¸ªæ‹¬å·
        
        values = df[time_cols].iloc[0].values
        threshold = 0.3
        
        truncate_pos = len(values)
        for i in range(1, len(values)):
            if pd.isna(values[i]) or pd.isna(values[i-1]):
                continue
            rate = abs(values[i] - values[i-1]) / (values[i-1] + 1e-6)
            if rate < threshold:
                truncate_pos = i
                break
        
        for col in time_cols[truncate_pos:]:
            df[col] = np.nan
        return df

    def _get_slope(self, row):
        x = np.arange(len(row))
        y = row.values
        mask = ~np.isnan(y)
        if sum(mask) >= 2:
            return stats.linregress(x[mask], y[mask])[0]
        return 0.0

    def _calc_autocorr(self, row):
        values = row.dropna().values
        if len(values) > 1:
            return np.corrcoef(values[:-1], values[1:])[0, 1]
        return 0.0

    def _extract_time_series_features(self, df):
        time_data = df[self.time_series_cols]
        time_data_filled = time_data.ffill(axis=1).bfill(axis=1)
        
        features = pd.DataFrame()
        features['seq_length'] = time_data_filled.notna().sum(axis=1)
        features['max_value'] = time_data_filled.max(axis=1)
        features['mean_value'] = time_data_filled.mean(axis=1)
        features['min_value'] = time_data_filled.min(axis=1)
        features['std_value'] = time_data_filled.std(axis=1)
        features['range_value'] = features['max_value'] - features['min_value']
        features['trend'] = time_data_filled.apply(self._get_slope, axis=1)
        features['autocorr'] = time_data_filled.apply(self._calc_autocorr, axis=1)
        return features.fillna(0)

    def predict_one(self, sample):
        # æ„å»ºè¾“å…¥æ•°æ®æ¡†æ¶
        full_cols = self.static_cols + self.time_series_cols
        df = pd.DataFrame([sample], columns=full_cols)
        
        # æ•°æ®é¢„å¤„ç†
        df = self._truncate(df)
        
        # ç‰¹å¾æå–
        static_features = df[self.static_cols]
        time_features = self._extract_time_series_features(df)
        
        # ç‰¹å¾åˆå¹¶ä¸å¯¹é½
        feature_df = pd.concat([static_features, time_features], axis=1)
        feature_df = feature_df.reindex(columns=self.expected_features, fill_value=0)
        
        # ç»´åº¦éªŒè¯
        if feature_df.shape[1] != len(self.expected_features):
            raise ValueError(f"ç‰¹å¾ç»´åº¦é”™è¯¯ï¼å½“å‰ï¼š{feature_df.shape[1]}ï¼Œéœ€è¦ï¼š{len(self.expected_features)}")
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.transform(feature_df)
        
        # é¢„æµ‹ä¸ç»“æœå¤„ç†
        prediction = self.model.predict(X_scaled)[0]
        proba = self.model.predict_proba(X_scaled)[0]
        return prediction, proba

# ==================== Streamlitç•Œé¢ ====================
def image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode()

# é¡µé¢é…ç½®
image_path = "å›¾ç‰‡1.png"
icon_base64 = image_to_base64(image_path)
st.set_page_config(
    page_title="èšä¸™çƒ¯LOIå’ŒTSæ¨¡å‹",
    layout="wide",
    page_icon=f"data:image/png;base64,{icon_base64}"
)

# é¡µé¢æ ‡é¢˜æ ·å¼
width = 200
height = int(158 * (width / 507))
st.markdown(
    f"""
    <h1 style="display: flex; align-items: center;">
        <img src="data:image/png;base64,{icon_base64}" style="width: {width}px; height: {height}px; margin-right: 15px;" />
        é˜»ç‡ƒèšåˆç‰©å¤åˆææ–™æ™ºèƒ½è®¾è®¡å¹³å°
    </h1>
    """, 
    unsafe_allow_html=True
)

# ä¾§è¾¹æ ä¸»å¯¼èˆª
page = st.sidebar.selectbox(
    "ğŸ”§ ä¸»åŠŸèƒ½é€‰æ‹©",
    ["é¦–é¡µ","æ€§èƒ½é¢„æµ‹", "é…æ–¹å»ºè®®"],
    key="main_nav"
)

# å­åŠŸèƒ½é€‰æ‹©
sub_page = None
if page == "é…æ–¹å»ºè®®":
    sub_page = st.sidebar.selectbox(
        "ğŸ”§ å­åŠŸèƒ½é€‰æ‹©",
        ["é…æ–¹ä¼˜åŒ–", "æ·»åŠ å‰‚æ¨è"],
        key="sub_nav"
    )

# åŠ è½½æ¨¡å‹
@st.cache_resource
def load_models():
    loi_data = joblib.load("model_and_scaler_loi.pkl")
    ts_data = joblib.load("model_and_scaler_ts1.pkl")
    return {
        "loi_model": loi_data["model"],
        "loi_scaler": loi_data["scaler"],
        "ts_model": ts_data["model"],
        "ts_scaler": ts_data["scaler"],
        "loi_features": pd.read_excel("trainrg3.xlsx").drop(columns="LOI", errors='ignore').columns.tolist(),
        "ts_features": pd.read_excel("trainrg3TS.xlsx").drop(columns="TS", errors='ignore').columns.tolist(),
    }
models = load_models()

# é¦–é¡µ
if page == "é¦–é¡µ":
    st.markdown("""
    æœ¬å¹³å°åŸºäºå…ˆè¿›çš„äººå·¥æ™ºèƒ½å’Œææ–™ç§‘å­¦æŠ€æœ¯ï¼Œè‡´åŠ›äºæä¾›èšä¸™çƒ¯ï¼ˆPPï¼‰ç­‰èšåˆç‰©å¤åˆææ–™çš„æ€§èƒ½é¢„æµ‹ä¸é…æ–¹ä¼˜åŒ–å»ºè®®ã€‚
    é€šè¿‡æœ¬å¹³å°ï¼Œç”¨æˆ·å¯ä»¥è¿›è¡Œææ–™æ€§èƒ½é¢„æµ‹ï¼ˆå¦‚LOIå’ŒTSé¢„æµ‹ï¼‰ï¼Œå¹¶æ ¹æ®æ€§èƒ½ç›®æ ‡ä¼˜åŒ–é…æ–¹ï¼Œæ¨èé€‚åˆçš„åŠ©å‰‚ã€‚
    """)
    st.markdown("<hr>", unsafe_allow_html=True)
    st.markdown("""
    ## åŠŸèƒ½æ¦‚è§ˆ
    1. **æ€§èƒ½é¢„æµ‹**ï¼šé€šè¿‡è¾“å…¥ææ–™é…æ–¹ï¼Œé¢„æµ‹èšåˆç‰©å¤åˆææ–™çš„LOIå’ŒTSæ€§èƒ½ã€‚
    2. **é…æ–¹å»ºè®®**ï¼šæ ¹æ®ç›®æ ‡æ€§èƒ½ï¼Œä¼˜åŒ–ææ–™é…æ–¹ã€‚
    3. **æ·»åŠ å‰‚æ¨è**ï¼šæ ¹æ®é»„åº¦å€¼ç­‰æ—¶åºæ•°æ®ï¼Œæ™ºèƒ½æ¨èæœ€ä½³æ·»åŠ å‰‚ã€‚
    """)
    st.markdown("<hr>", unsafe_allow_html=True)
    st.markdown("""
    ## **å¼•ç”¨**
    Weibin, Ma; Ling, Li; Yu, Zhang et al. Active learning-based generative design of halogen-free flame-retardant polymeric composites. Journal of Materials Informatics
    """)
    st.markdown("""
    ## **è‡´è°¢**<br>
    *è´¡çŒ®è€…*ï¼š<br>
    *å›¢é˜Ÿ*ï¼š<br>
    ä¸Šæµ·å¤§å­¦åŠŸèƒ½é«˜åˆ†å­ç»„<br>
    *å¼€å‘è€…*ï¼š<br>
    é©¬ç»´å®¾åšå£«ç”Ÿ<br>
    *å®¡æŸ¥*ï¼š<br>
    ä¸é¹æ•™æˆ<br>
    *åŸºé‡‘æ”¯æŒ*ï¼š<br>
    äº‘å—çœç§‘æŠ€é‡ç‚¹è®¡åˆ’é¡¹ç›® ï¼ˆ202302AB080022ï¼‰ã€è‹å·å¸‚é‡ç‚¹æŠ€æœ¯ç ”ç©¶é¡¹ç›® ï¼ˆSYG2024017ï¼‰
    """, unsafe_allow_html=True)
    st.markdown("<hr>", unsafe_allow_html=True)

# æ€§èƒ½é¢„æµ‹é¡µé¢
elif page == "æ€§èƒ½é¢„æµ‹":
    st.subheader("ğŸ”® æ€§èƒ½é¢„æµ‹ï¼šåŸºäºé…æ–¹é¢„æµ‹LOIå’ŒTS")
    
    matrix_materials = ["PP", "PA", "PC/ABS", "POM", "PBT", "PVC", "å…¶ä»–"]
    flame_retardants = [
        "AHP", "ammonium octamolybdate", "Al(OH)3", "CFA", "APP", "Pentaerythritol", "DOPO", 
        "EPFR-1100NT", "XS-FR-8310", "ZS", "XiuCheng", "ZHS", "ZnB", "antimony oxides", 
        "Mg(OH)2", "TCA", "MPP", "PAPP", "å…¶ä»–"
    ]
    additives = [
        "wollastonite", "M-2200B", "ZBS-PV-OA", "FP-250S", "silane coupling agent", "antioxidant", 
        "SiO2", "å…¶ä»–"
    ]
    
    fraction_type = st.sidebar.selectbox("é€‰æ‹©è¾“å…¥çš„å•ä½", ["è´¨é‡", "è´¨é‡åˆ†æ•°", "ä½“ç§¯åˆ†æ•°"])

    st.subheader("è¯·é€‰æ‹©é…æ–¹ä¸­çš„åŸºä½“ã€é˜»ç‡ƒå‰‚å’ŒåŠ©å‰‚")
    selected_matrix = st.selectbox("é€‰æ‹©åŸºä½“", matrix_materials, index=0)
    selected_flame_retardants = st.multiselect("é€‰æ‹©é˜»ç‡ƒå‰‚", flame_retardants, default=["ZS"])
    selected_additives = st.multiselect("é€‰æ‹©åŠ©å‰‚", additives, default=["wollastonite"])
    
    input_values = {}
    unit = get_unit(fraction_type)
    
    input_values[selected_matrix] = st.number_input(f"é€‰æ‹© {selected_matrix} ({unit})", min_value=0.0, max_value=100.0, value=50.0, step=0.1)
    
    for fr in selected_flame_retardants:
        input_values[fr] = st.number_input(f"é€‰æ‹© {fr} ({unit})", min_value=0.0, max_value=100.0, value=10.0, step=0.1)
    
    for ad in selected_additives:
        input_values[ad] = st.number_input(f"é€‰æ‹© {ad} ({unit})", min_value=0.0, max_value=100.0, value=10.0, step=0.1)
    
    total = sum(input_values.values())
    is_only_pp = all(v == 0 for k, v in input_values.items() if k != "PP")
    
    with st.expander("âœ… è¾“å…¥éªŒè¯"):
        if fraction_type in ["ä½“ç§¯åˆ†æ•°", "è´¨é‡åˆ†æ•°"]:
            if abs(total - 100.0) > 1e-6:
                st.error(f"â— {fraction_type}çš„æ€»å’Œå¿…é¡»ä¸º100%ï¼ˆå½“å‰ï¼š{total:.2f}%ï¼‰")
            else:
                st.success(f"{fraction_type}æ€»å’ŒéªŒè¯é€šè¿‡")
        else:
            st.success("æˆåˆ†æ€»å’ŒéªŒè¯é€šè¿‡")
            if is_only_pp:
                st.info("æ£€æµ‹åˆ°çº¯PPé…æ–¹")

    if st.button("ğŸš€ å¼€å§‹é¢„æµ‹", type="primary"):
        if fraction_type in ["ä½“ç§¯åˆ†æ•°", "è´¨é‡åˆ†æ•°"] and abs(total - 100.0) > 1e-6:
            st.error(f"é¢„æµ‹ä¸­æ­¢ï¼š{fraction_type}çš„æ€»å’Œå¿…é¡»ä¸º100%")
            st.stop()

        if is_only_pp:
            loi_pred, ts_pred = 17.5, 35.0
        else:
            # ...ï¼ˆä¿æŒåŸæœ‰æ•°æ®å¤„ç†é€»è¾‘ï¼‰...
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric(label="LOIé¢„æµ‹å€¼", value=f"{loi_pred:.2f}%")
        with col2:
            st.metric(label="TSé¢„æµ‹å€¼", value=f"{ts_pred:.2f} MPa")

# é…æ–¹å»ºè®®é¡µé¢
elif page == "é…æ–¹å»ºè®®":
    if sub_page == "é…æ–¹ä¼˜åŒ–":
        # ...ï¼ˆä¿æŒåŸæœ‰é…æ–¹ä¼˜åŒ–ä»£ç ï¼‰...
    
    elif sub_page == "æ·»åŠ å‰‚æ¨è":
        st.subheader("ğŸ§ª PVCæ·»åŠ å‰‚æ™ºèƒ½æ¨è")
        predictor = Predictor("scaler_fold_1.pkl", "svc_fold_1.pkl")
        
        with st.form("additive_form"):
            # ...ï¼ˆä¿æŒåŸæœ‰è¡¨å•ä»£ç ï¼‰...
        
        if submit_btn:
            try:
                # ...ï¼ˆä¿æŒåŸæœ‰é¢„æµ‹å¤„ç†ä»£ç ï¼‰...
            except Exception as e:
                st.error(f"é¢„æµ‹é”™è¯¯ï¼š{str(e)}")
                st.stop()

# é¡µè„š
def add_footer():
    st.markdown("""
    <hr>
    <footer style="text-align: center;">
        <p>Â© 2025 é˜»ç‡ƒèšåˆç‰©å¤åˆææ–™æ™ºèƒ½è®¾è®¡å¹³å°</p>
        <p>å¼€å‘è€…: é©¬ç»´å®¾</p>
        <p>å¹³å°æ€§è´¨å£°æ˜ï¼šæœ¬å¹³å°ä¸ºç§‘ç ”åä½œç½‘ç»œæœåŠ¡å¹³å°ï¼Œæ‰€æœ‰å†…å®¹ä»…ä¾›å­¦æœ¯ç ”ç©¶ã€æŠ€æœ¯éªŒè¯ç­‰éè¥åˆ©æ€§ç§‘ç ”æ´»åŠ¨ä½¿ç”¨ï¼Œä¸¥ç¦ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚</p>
    </footer>
    """, unsafe_allow_html=True)
add_footer()
