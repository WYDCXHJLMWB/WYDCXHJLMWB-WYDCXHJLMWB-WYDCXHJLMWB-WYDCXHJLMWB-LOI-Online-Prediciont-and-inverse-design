import numpy as np
import pandas as pd
import streamlit as st
from deap import base, creator, tools
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

# åŠ è½½æ¨¡å‹å’ŒScalerï¼ˆå‡è®¾æ‚¨å·²ç»æœ‰è¿™äº›è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
model = LinearRegression()  # å‡è®¾å·²ç»è®­ç»ƒå¥½
scaler = StandardScaler()  # å‡è®¾å·²ç»è®­ç»ƒå¥½

# å‡è®¾æœ‰é…æ–¹æˆåˆ†ç‰¹å¾
feature_names = ['æˆåˆ†1', 'æˆåˆ†2', 'æˆåˆ†3', 'æˆåˆ†4', 'æˆåˆ†5']

# å‡è®¾ç›®æ ‡LOI
target_loi = 50  # ç›®æ ‡LOIå€¼

# åˆ›å»ºé€‚åº”åº¦å’Œä¸ªä½“ç±»
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)

# è®¾ç½®é—ä¼ ç®—æ³•å·¥å…·ç®±
toolbox = base.Toolbox()
toolbox.register("attr_float", np.random.uniform, 0.01, 0.5)  # è®¾ç½®æœ€å°å€¼ä¸º0.01ï¼Œé¿å…è´Ÿæ•°
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, n=len(feature_names))
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# ä¿®æ”¹é€‚åº”åº¦è¯„ä¼°å‡½æ•°
def evaluate(individual):
    user_input = dict(zip(feature_names, individual))
    
    # ä¿è¯é…æ–¹æ€»å’Œä¸º100ï¼Œå¿…è¦æ—¶è¿›è¡Œè°ƒæ•´
    total = sum(user_input.values())
    if total != 100:
        user_input = {k: (v / total) * 100 for k, v in user_input.items()}  # å½’ä¸€åŒ–ä¸ºè´¨é‡åˆ†æ•°
    
    # ä½¿ç”¨æ¨¡å‹è¿›è¡ŒLOIé¢„æµ‹
    input_array = np.array([list(user_input.values())])
    input_scaled = scaler.transform(input_array)
    predicted_loi = model.predict(input_scaled)[0]
    
    return abs(predicted_loi - target_loi),

toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)
toolbox.register("evaluate", evaluate)

# ä¿®æ”¹ä¸ªä½“ç”Ÿæˆæ–¹å¼ï¼Œç¡®ä¿ç”Ÿæˆçš„ä¸ªä½“æ€»å’Œä¸º100ï¼Œä¸”ç¬¬ä¸€åˆ—å«é‡æœ€å¤š
def create_individual():
    individual = np.random.uniform(0.01, 0.5, len(feature_names))  # ç”Ÿæˆ0.01åˆ°0.5ä¹‹é—´çš„å€¼
    individual[0] = max(individual[0], 50.0)  # ç¡®ä¿ç¬¬ä¸€åˆ—çš„å€¼å¤§äºç­‰äº50
    total = sum(individual)
    individual = (individual / total) * 100  # ç¡®ä¿æ€»å’Œä¸º100
    return individual

population = [create_individual() for _ in range(100)]

# è¿è¡Œé—ä¼ ç®—æ³•
for gen in range(100):
    offspring = toolbox.select(population, len(population))
    offspring = list(map(toolbox.clone, offspring))

    for child1, child2 in zip(offspring[::2], offspring[1::2]):
        if np.random.random() < 0.7:  # 70%çš„æ¦‚ç‡äº¤å‰
            toolbox.mate(child1, child2)
            del child1.fitness.values  # åˆ é™¤é€‚åº”åº¦ï¼Œå‡†å¤‡é‡æ–°è¯„ä¼°
            del child2.fitness.values  # åˆ é™¤é€‚åº”åº¦ï¼Œå‡†å¤‡é‡æ–°è¯„ä¼°

    for mutant in offspring:
        if np.random.random() < 0.2:  # 20%çš„æ¦‚ç‡å˜å¼‚
            toolbox.mutate(mutant)
            del mutant.fitness.values  # åˆ é™¤é€‚åº”åº¦ï¼Œå‡†å¤‡é‡æ–°è¯„ä¼°

    # é‡æ–°è¯„ä¼°ä¸ªä½“çš„é€‚åº”åº¦
    invalid_individuals = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = map(toolbox.evaluate, invalid_individuals)
    for ind, fit in zip(invalid_individuals, fitnesses):
        ind.fitness.values = fit

    population[:] = offspring

# è·å–æœ€ä¼˜è§£å¹¶è¾“å‡ºä¸ºæ•°æ®æ¡†æ ¼å¼
best_individual = tools.selBest(population, 1)[0]
best_result = dict(zip(feature_names, best_individual))

# å°†ç»“æœè½¬æ¢ä¸ºæ•°æ®æ¡†
result_df = pd.DataFrame(list(best_result.items()), columns=["æˆåˆ†", "è´¨é‡åˆ†æ•° (wt%)"])

# æ˜¾ç¤ºé…æ–¹å»ºè®®
st.markdown("### ğŸ¯ å»ºè®®é…æ–¹")
st.dataframe(result_df)
